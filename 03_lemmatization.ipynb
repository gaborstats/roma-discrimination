{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tartalom:\n",
    "\n",
    "1. lexikai gyakorisagok \n",
    "2. lemmatizalas\n",
    "3. lexikai diverzitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install https://github.com/oroszgy/spacy-hungarian-models/releases/download/hu_core_ud_lg-0.3.1/hu_core_ud_lg-0.3.1-py3-none-any.whl  \n",
    "# kb 50 percig fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hu_core_ud_lg\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "import hu_core_ud_lg\n",
    "from lexical_diversity import lex_div as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2020-08-30_reply.csv',  sep = \";\", decimal = \",\", encoding = \"ISO-8859-2\", skipinitialspace=True)\n",
    "\n",
    "df[\"reply\"].replace({'ĺ\\x90': 'Ő'}, inplace=True, regex=True)\n",
    "df[\"reply\"].replace({'í\\xad': 'í'}, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. gyakorisagok szamolasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "szamok = []\n",
    "mondatokszama = []\n",
    "szavakszama = []\n",
    "atlagosmondathossz = []\n",
    "pont = []\n",
    "vesszo = []\n",
    "irj = []\n",
    "felk = []\n",
    "emoji = []\n",
    "\n",
    "for text in df.reply:\n",
    "    \n",
    "    szamok.append(len(re.findall('[0-9]', text)))  # szamok karakterhossza\n",
    "    \n",
    "    sent_fre = 0\n",
    "    for sent in nltk.sent_tokenize(text): # mondatok szama\n",
    "        sent_fre += 1\n",
    "    mondatokszama.append(sent_fre)\n",
    "    \n",
    "    szavakszama.append(len(re.findall(r'\\w+', text))) # szavak szama az emailben    \n",
    "  \n",
    "    pont.append(len(re.findall('\\.', text))) # pontok szama\n",
    "    \n",
    "    vesszo.append(len(re.findall(',', text)))  # vesszok szama\n",
    "    \n",
    "    felk.append(len(re.findall('!', text)))  # felkialtojelek szama\n",
    "    \n",
    "    irj.append(len(re.findall('[^.\\w,]', re.sub(' ', '', text)))) # irasjelek szama (nem ., vagy szokoz)\n",
    "    \n",
    "    emoji.append(len(re.findall(':\\)|:-\\)|;-\\)|;\\)|:-D|:D|;-D|;D|:-\\(', text)) ) # ezt \":(\" kivettem, mert zarojelnel szerepelt\n",
    "    # az emjoi python fv-t nem sikerult beuzemeltetni, mert unicode kell neki.\n",
    "\n",
    "df[\"szam_char\"] = szamok\n",
    "df[\"mondatok\"] = mondatokszama\n",
    "df[\"szavak\"] = szavakszama\n",
    "df[\"mondatok_avg\"] = df[\"mondatok\"] / df[\"szavak\"] \n",
    "df[\"pontok\"] = pont\n",
    "df[\"vesszok\"] = vesszo\n",
    "df[\"felk\"] = felk\n",
    "df[\"irj\"] = irj\n",
    "df[\"emoji\"] = emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. lemmatizalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = hu_core_ud_lg.load()\n",
    "# 2-3 perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lemma = []\n",
    "\n",
    "for i in df.reply:\n",
    "    mondat = \"\"\n",
    "    doc = nlp(i)\n",
    "    newtext = [(tok.lemma_, tok.is_title) for tok in doc]\n",
    "    mondat = ' '.join([tok[0].title() if tok[1] == 1 else tok[0] for tok in newtext])\n",
    "    output_lemma.append(mondat)\n",
    "\n",
    "df.loc[:,\"lemma\"] = output_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kell a whitestrip a diverzitas stathoz\n",
    "lemma_wt_whitespace = []\n",
    "\n",
    "for text in df.lemma:\n",
    "    text = text.lstrip()\n",
    "    # remove whitespace after punctuation and before/after parentheses\n",
    "    lemma_wt_whitespace.append(re.sub(r'(\\s([,.:;!?\"]))|(?<=\\[|\\()(.*?)(?=\\)|\\])', lambda x: x.group().strip(), text))\n",
    "\n",
    "df[\"lemma\"] = lemma_wt_whitespace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. lexikai diverzitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = []\n",
    "basic = []\n",
    "simple_ttr = []\n",
    "root_ttr = []\n",
    "log_ttr = []\n",
    "\n",
    "for text in df.lemma:\n",
    "    tok = ld.tokenize(text)\n",
    "    basic.append(len(tok) / len(set(tok)) ) # ez a simple_ttr reciprokja, azert jo mert mas iranyba nyulik el az eloszlas? \n",
    "    simple_ttr.append(ld.ttr(tok)) # unique / total\n",
    "    root_ttr.append(ld.root_ttr(tok)) # unique / sqrt(total)\n",
    "    log_ttr.append(ld.log_ttr(tok)) # ln(unique) / ln(total)\n",
    "\n",
    "df[\"basic\"] = basic\n",
    "df[\"simple_ttr\"] = simple_ttr\n",
    "df[\"root_ttr\"] = root_ttr\n",
    "df[\"log_ttr\"] = log_ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"2020-09-02_lemmatized.csv\", sep = \";\", encoding = \"ISO-8859-2\", header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
